#pragma kernel LeakyReLU THREADS=32
#pragma kernel BatchNormalization THREADS=32 
#pragma kernel InputLayer THREADS=32
#pragma kernel OutputLayer  THREADS=32
#pragma kernel UpSampling2D THREADS=32
#pragma kernel ReLU THREADS=32
#pragma kernel Tanh THREADS=32 
#pragma kernel Concatenate THREADS=32
#pragma kernel Conv2D_8 THREADS=8
#pragma kernel Conv2D_11 THREADS=11
#pragma kernel Conv2D_16 THREADS=16
#pragma kernel Conv2D_19 THREADS=19
#pragma kernel Conv2D_32 THREADS=32
#pragma kernel Conv2D_64 THREADS=64

Texture2D<float3> InputImage;
Texture2D<float3> InputImage1;
RWTexture2D<float3> OutputImage;

Buffer<float> LayerInput0;
Buffer<float> LayerInput1;
RWBuffer<float> LayerOutput;
Buffer<float> Weights;
uint4 WeightsShape;//for conv2d: n_Hk, n_Wk, n_Ci, n_Ck; for other: size, 0, 0, 0
uint2 Stride;
uint3 InputShape;//n_Hi, n_Wi, n_Ci
uint3 OutputShape;//n_Ho, n_Wo, n_Co=n_Ck
uint3 InputShapeIdMultiplier;
uint3 InputShapeIdMultiplier1;
uint3 OutputShapeIdMultiplier;
uint4 WeightsShapeIdMultiplier;
uint2 Size;
float Alpha;

#define n_Wk WeightsShape.y
#define n_Hk WeightsShape.x
#define n_Ck WeightsShape.w
#define n_Wi InputShape.y
#define n_Hi InputShape.x
#define n_Ci InputShape.z
#define n_Wo OutputShape.y
#define n_Ho OutputShape.x
#define n_Co OutputShape.z

groupshared float cache[THREADS*8];

[numthreads(8, 1, THREADS)]
#define KERNEL_NAME(x, y) x##_##y
void KERNEL_NAME(Conv2D, THREADS)
(uint3 id : SV_DispatchThreadID, uint3 groupid : SV_GroupThreadID)
{
	//id: outputshape
	uint2 InputId = id.xy * Stride;

	float bias = Weights[n_Wk * n_Hk * n_Ci * n_Ck + id.z];
	float conv = 0;

	uint3 offset = uint3((n_Wk - 1) / 2, (n_Hk - 1) / 2, 0);
	
	//each kernel x
	for (uint p = 0; p < n_Wk; p++) {
		//each kernel y
		for (uint q = 0; q < n_Hk; q++) {
			
			int3 input_id = int3(InputId.yx, id.z) - offset + int3(p, q, 0);

			if (id.z < n_Ci) {
				if (any(input_id.xy < 0) || any(input_id.xy > InputShape.yx)) {
					cache[groupid.x * THREADS + id.z] = 0;
				}
				else {
					cache[groupid.x * THREADS + id.z] = LayerInput0[dot(input_id.yxz, InputShapeIdMultiplier)];
				}
			}
			
			GroupMemoryBarrierWithGroupSync();
			
			//each layer input, n_Ci = kernel z
			for (uint w = 0; w < n_Ci; w++) {
				conv += cache[groupid.x * THREADS + w] * Weights[dot(uint4(q, p, w, id.z), WeightsShapeIdMultiplier)];
			}

			GroupMemoryBarrierWithGroupSync();
		}
	}
	LayerOutput[dot(id.xyz, OutputShapeIdMultiplier)] = conv + bias;
}

[numthreads(32, 1, 1)]
void ReLU(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = LayerInput0[id.x] > 0 ? LayerInput0[id.x] : 0;
}

[numthreads(32, 1, 1)]
void Tanh(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = tanh(LayerInput0[id.x]);
}

[numthreads(32, 1, 1)]
void LeakyReLU(uint3 id : SV_DispatchThreadID)
{
	LayerOutput[id.x] = LayerInput0[id.x] > 0 ? LayerInput0[id.x] : LayerInput0[id.x] * Alpha;
}

[numthreads(8, 8, 1)]
void Concatenate(uint3 id : SV_DispatchThreadID)
{
	if (id.z < InputShape.z) {
		LayerOutput[dot(id, OutputShapeIdMultiplier)] = LayerInput0[dot(id, InputShapeIdMultiplier)];
	}
	else {
		LayerOutput[dot(id, OutputShapeIdMultiplier)] = LayerInput1[dot(uint3(id.xy, id.z - InputShape.z), InputShapeIdMultiplier1)];
	}
}

//momentum trained, take parameter as population mean/std is ok
[numthreads(32, 1, 1)]
void BatchNormalization(const uint3 id : SV_DispatchThreadID)
{
	float gamma		= Weights[id.y * 4];
	float beta		= Weights[id.y * 4 + 1];
	float mean	= Weights[id.y * 4 + 2];
	float std	= Weights[id.y * 4 + 3];

	const uint stride = n_Ci;

	uint idx = id.x * stride + id.y;
	LayerOutput[idx] = (LayerInput0[idx] - mean) * gamma / sqrt(std + 1e-8) + beta;
}

[numthreads(8, 8, 1)]
void UpSampling2D(uint3 id : SV_DispatchThreadID)
{
	float2 inputid = (float2)id.xy / (float2)Size.xy;
	uint3 floor_inputid = uint3(floor(inputid), id.z);
	float2 frac_inputid = inputid - floor_inputid.xy;
	float bilinear_interp =
		LayerInput0[dot(floor_inputid, InputShapeIdMultiplier)] * (1 - frac_inputid.x) * (1 - frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(1, 0, 0), InputShapeIdMultiplier)] * (frac_inputid.x) * (1 - frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(0, 1, 0), InputShapeIdMultiplier)] * (1 - frac_inputid.x) * (frac_inputid.y) +
		LayerInput0[dot(floor_inputid + uint3(1, 1, 0), InputShapeIdMultiplier)] * (frac_inputid.x) * (frac_inputid.y);
	LayerOutput[dot(id, OutputShapeIdMultiplier)] = bilinear_interp;
}

[numthreads(8, 8, 1)]
void InputLayer(uint3 id : SV_DispatchThreadID)
{
	uint2 remapid = uint2(id.y, InputShape.x - 1 - id.x);
	float3 remap = InputImage[remapid.xy] * 2.0f - 1.0f;
	float3 remap_dep = InputImage1[remapid.xy] * 2.0f - 1.0f;
	LayerOutput[dot(uint3(id.xy, 0), InputShapeIdMultiplier)] = remap.x;
	LayerOutput[dot(uint3(id.xy, 1), InputShapeIdMultiplier)] = remap.y;
	LayerOutput[dot(uint3(id.xy, 2), InputShapeIdMultiplier)] = remap.z;
	LayerOutput[dot(uint3(id.xy, 3), InputShapeIdMultiplier)] = remap_dep.x;
}

[numthreads(8, 8, 1)]
void OutputLayer(uint3 id : SV_DispatchThreadID)
{
	uint2 remapid = uint2(id.y, InputShape.x - 1 - id.x);
	OutputImage[remapid.xy] = saturate(float3(
		LayerInput0[dot(uint3(id.xy, 0), InputShapeIdMultiplier)] * 0.5f + 0.5f,
		LayerInput0[dot(uint3(id.xy, 1), InputShapeIdMultiplier)] * 0.5f + 0.5f,
		LayerInput0[dot(uint3(id.xy, 2), InputShapeIdMultiplier)] * 0.5f + 0.5f));
}